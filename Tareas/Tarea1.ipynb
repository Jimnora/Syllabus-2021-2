{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tarea 1\r\n",
    "\r\n",
    "En esta tarea vas a programar tu mismo el algoritmo de descenso del gradiente, en dos versiones, y comparar ambos resultados. Para probar tu modelo vas a tener que usar el _dataset_ de gustos de helados que vimos en clases.\r\n",
    "\r\n",
    "## Parte 1 (3 pts): gradient descent simple\r\n",
    "\r\n",
    "En el codigo de abajo hay una clase llamada `LogisticRegression` cuyo constructor recibe como parámetro el número de _features_ que espera recibir. Tienes que completar esta clase para que pueda entrenar y predecir. Lo que necesitas es:\r\n",
    "\r\n",
    "- Programar el método `train`, que vendría a ser equivalente al método `fit` de Scikit Learn. Tienes que utilizar el algoritmo _Gradient Descent_ visto en clases.\r\n",
    "- Programar el método `predict` que asume que tu modelo ya está entrenado.\r\n",
    "\r\n",
    "Para hacer esto puedes hacer los supuestos razonables que estimes conveniente. Además, si te acomoda trabajar sin clases puedes hacerlo, mientras uses el algoritmo de _Gradient Descent_. **Importante**: puedes asumir que una instancia es \"positiva\" (es decir, pertenece a la clase 1) si la probabilidad calculada es mayor o igual a 0.5.\r\n",
    "\r\n",
    "Recuerda además que el gradiente de la función objetivo para cada $\\beta_i$ es:\r\n",
    "\r\n",
    "$$\r\n",
    "\\frac{\\delta}{\\delta \\beta_i}L(\\beta) = \\frac{1}{m} \\sum_{1 \\leq j \\leq m} (\\sigma(\\beta^T x^j) - y_j) x_i^j\r\n",
    "$$\r\n",
    "\r\n",
    "Donde $L(\\beta)$ es la función de verosimilitud, $\\beta$ es el vector de coeficientes para la regresión, tenemos $m$ filas en nuestro _dataset_, $\\sigma(x)$ es la función $\\frac{1}{1 + e^{-x}}$, $x^j$ es la fila $j$ de nuestro dataset (y asociado tiene su respuesta $y_j$) y finalmente $x_i^j$ es la columna $i$ de la fila $j$ en nuestro _dataset_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# Tienes que programar la parte 1 aquí\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "# La función sigmoide\r\n",
    "def sigmoid(x):    \r\n",
    "    output = 1 / (1 + np.exp(-x))\r\n",
    "    return output\r\n",
    "\r\n",
    "\r\n",
    "class LogisticRegression:\r\n",
    "    def __init__(self, number_of_features, learning_rate=0.001, number_of_iterations=100):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.number_of_iterations = number_of_iterations\r\n",
    "        self.beta = np.random.randn(number_of_features, 1)\r\n",
    "        self.b = 0\r\n",
    "    \r\n",
    "    def _map_binario(self, pred):\r\n",
    "        # Funcion para realizar el mapeo a 0 y 1 de los valores de la regresion\r\n",
    "        if pred >= 0.5:\r\n",
    "            return 1\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "        \r\n",
    "    def train(self, X, y):\r\n",
    "        # Se cambia la serie de pandas a un array de numpy\r\n",
    "        y = y.to_numpy()\r\n",
    "\r\n",
    "        # Se realiza por cada iteracion\r\n",
    "        for iteration in range(self.number_of_iterations):\r\n",
    "            #Se calcula las predicciones con la funcion sigmoide\r\n",
    "            Z=np.dot(self.beta.T,X.T) + self.b\r\n",
    "            y_pred = sigmoid(Z)\r\n",
    "\r\n",
    "            #Se calcula las gradientes y B0\r\n",
    "            gradients=1/len(X)*np.dot(X.T,(y_pred-y).T)\r\n",
    "            constant=1/len(X)*np.sum(y_pred-y)\r\n",
    "\r\n",
    "            #Se reajusta los valores\r\n",
    "            self.beta = self.beta - self.learning_rate * gradients\r\n",
    "            self.b = self.b - self.learning_rate * constant\r\n",
    "        pass\r\n",
    "    \r\n",
    "    def predict(self, value):\r\n",
    "        #Se hace las predicciones con la funcion sigmoide\r\n",
    "        Z=np.dot(self.beta.T,value.T) + self.b\r\n",
    "        y_pred = sigmoid(Z)\r\n",
    "\r\n",
    "        #Se hace un mapeo donde los valores >= 0.5 cambian a 1 y el resto cambian a 0\r\n",
    "        res = np.array([self._map_binario(pred) for pred in y_pred[0]])\r\n",
    "        \r\n",
    "        return res\r\n",
    "        pass\r\n",
    "\r\n",
    "# Ejemplo de uso de la clase para 3 features\r\n",
    "log_reg = LogisticRegression(3)\r\n",
    "print(log_reg.beta)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-0.45479995]\n",
      " [ 1.37636698]\n",
      " [ 0.21892173]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parte 2 (1 pto): entrenando el modelo, computando el log loss\r\n",
    "\r\n",
    "En esta parte tendrás que hacer un clasificador de gustos de helados tal como lo hicimos para regresión logística. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "import pandas as pd\r\n",
    "from sklearn.metrics import log_loss\r\n",
    "helados = pd.read_csv('Ice_cream.csv')\r\n",
    "helados.info()\r\n",
    "\r\n",
    "X = helados[['female','video','puzzle']]\r\n",
    "y = helados['ice_cream']\r\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\r\n",
    "log_reg.train(X, y)\r\n",
    "# Ahora usa el predict\r\n",
    "# predicciones = log_reg.predict(X, y)\r\n",
    "predicciones = log_reg.predict(X)\r\n",
    "print(log_reg.beta)\r\n",
    "#y obtiene el log loss\r\n",
    "print(\"\\nVALOR LOG LOSS:\")\r\n",
    "log_loss(y, predicciones)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Unnamed: 0  200 non-null    int64\n",
      " 1   id          200 non-null    int64\n",
      " 2   female      200 non-null    int64\n",
      " 3   ice_cream   200 non-null    int64\n",
      " 4   video       200 non-null    int64\n",
      " 5   puzzle      200 non-null    int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 9.5 KB\n",
      "[[-0.46200683]\n",
      " [ 0.45202139]\n",
      " [-0.44190163]]\n",
      "\n",
      "VALOR LOG LOSS:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15.88802904504216"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parte 3 (1.5 ptos) mini-batch gradient descent\n",
    "\n",
    "Otra forma de implementar el _gradient descent_ es mediante batches, o pedazos. La idea es la siguiente: \n",
    "- Se selecciona un número B de pedazos a usar y un número E de épocas\n",
    "- Para cada época: \n",
    "    - Dividir el set de entrenamiento en B pedazos \n",
    "    - Para cada pedazo: \n",
    "        - calcular el gradiente usando la fórmula, pero solo para los datos de ese pedazo. \n",
    "        - realizar una iteración del gradient descent y actualizar los coeficientes\n",
    "        \n",
    "En esta última parte, modifica tu clase LogisticRegression para que el train sea hecho con mini-bath gradient descent. Las épocas y los batches deben ser pasados como parámetros. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "# Tienes que programar la parte 3 aquí\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# La función sigmoide\r\n",
    "def sigmoid(x):    \r\n",
    "    output = 1 / (1 + np.exp(-x))\r\n",
    "    return output\r\n",
    "\r\n",
    "class LogisticRegressionBatches:\r\n",
    "    def __init__(self, number_of_features,n_batches,n_epochs, learning_rate=0.001):\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.n_batches = n_batches\r\n",
    "        self.n_epochs = n_epochs\r\n",
    "        self.beta = np.random.randn(number_of_features, 1)\r\n",
    "        self.b = 0\r\n",
    "\r\n",
    "    def _map_binario(self, pred):\r\n",
    "        # Funcion para realizar el mapeo a 0 y 1 de los valores de la regresion\r\n",
    "        if pred >= 0.5:\r\n",
    "            return 1\r\n",
    "        else:\r\n",
    "            return 0\r\n",
    "        \r\n",
    "    def train(self, X, y):\r\n",
    "        X = X.to_numpy()\r\n",
    "        y = y.to_numpy()\r\n",
    "\r\n",
    "        # Se realiza por cada epoca\r\n",
    "        for iteration in range(self.n_epochs):\r\n",
    "\r\n",
    "            # Metodo de shuffle obtenido de https://towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307\r\n",
    "            shuffle = list(np.random.permutation(len(X)))\r\n",
    "            sX = X[shuffle,:]\r\n",
    "            sy = y[shuffle]\r\n",
    "\r\n",
    "            batchesx = np.array_split(X,self.n_batches)\r\n",
    "            batchesy = np.array_split(y,self.n_batches)\r\n",
    "\r\n",
    "            for batch in range(self.n_batches):\r\n",
    "                segmento = batchesx[batch]\r\n",
    "                resultados = batchesy[batch]\r\n",
    "\r\n",
    "                #Se calcula las predicciones con la funcion sigmoide\r\n",
    "                Z=np.dot(self.beta.T,segmento.T) + self.b\r\n",
    "                y_pred = sigmoid(Z)\r\n",
    "\r\n",
    "                #Se calcula las gradientes y B0\r\n",
    "                gradients=1/len(segmento)*np.dot(segmento.T,(y_pred-resultados).T)\r\n",
    "                constant=1/len(segmento)*np.sum(y_pred-resultados)\r\n",
    "\r\n",
    "                #Se reajusta los valores\r\n",
    "                self.beta = self.beta - self.learning_rate * gradients\r\n",
    "                self.b = self.b - self.learning_rate * constant\r\n",
    "\r\n",
    "\r\n",
    "        pass\r\n",
    "    \r\n",
    "    def predict(self, value):\r\n",
    "        Z=np.dot(self.beta.T,value.T) + self.b\r\n",
    "        y_pred = sigmoid(Z)\r\n",
    "\r\n",
    "        #Se hace un mapeo donde los valores >= 0.5 cambian a 1 y el resto cambian a 0\r\n",
    "        res = np.array([self._map_binario(pred) for pred in y_pred[0]])\r\n",
    "        \r\n",
    "        return res\r\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parte 4 (0.5 pts): número de batches y épocas?\n",
    "\n",
    "Lo último que debes hacer es encontrar un número de batches y épocas de forma que el modelo que entrenas con esos batches y épocas entregue un log-loss similar al que obtuviste con el método de gradient descent en la parte 1-2. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "source": [
    "helados = pd.read_csv('Ice_cream.csv')\r\n",
    "helados.info()\r\n",
    "\r\n",
    "mb_log_reg = LogisticRegressionBatches(3,5,100)\r\n",
    "\r\n",
    "\r\n",
    "X = helados[['female','video','puzzle']]\r\n",
    "y = helados['ice_cream']\r\n",
    "# Entrena el modelo acá, debería ser como la siguiente línea\r\n",
    "mb_log_reg.train(X, y)\r\n",
    "\r\n",
    "# Ahora usa el predict\r\n",
    "# predicciones = log_reg.predict(X, y)\r\n",
    "predicciones = mb_log_reg.predict(X)\r\n",
    "print(mb_log_reg.beta)\r\n",
    "#y obtiene el log loss\r\n",
    "print(\"\\nVALOR LOG LOSS:\")\r\n",
    "log_loss(y, predicciones)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   Unnamed: 0  200 non-null    int64\n",
      " 1   id          200 non-null    int64\n",
      " 2   female      200 non-null    int64\n",
      " 3   ice_cream   200 non-null    int64\n",
      " 4   video       200 non-null    int64\n",
      " 5   puzzle      200 non-null    int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 9.5 KB\n",
      "[[ 0.18692105]\n",
      " [ 0.21974854]\n",
      " [-0.2169118 ]]\n",
      "\n",
      "VALOR LOG LOSS:\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15.888037041016462"
      ]
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Detalles académicos\n",
    "\n",
    "La entrega de este control debe ser un solo archivo **Jupyter Notebook**. **La fecha de entrega es hasta el viernes 10 de Septiembre, hasta las 20:00 pm**. La nota se calcula como el número de puntos + un punto base. El archivo se entrega en un cuestionario en siding. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "02de5609ef162b6bcc51dd429fd31b9a1f163c414526741362218fd7f2e354ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}